{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "movie_reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "movie_reviews.isnull().values.any()\n",
    "\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26706c716a0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='sentiment', data=movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dark1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "STOPWORDS.add('br')\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "   \n",
    "    remove_tags(text)  \n",
    "    # text = re.sub('[^a-zA-Z]', ' ', text)# Remove punctuations and numbers\n",
    "    # text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)# Single character removal\n",
    "    # text = re.sub(r'\\s+', ' ', text)# Removing multiple spaces\n",
    "    text = text.lower()# lowercase text  \n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text    \n",
    "    text = re.sub(BAD_SYMBOLS_RE,' ',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    text = ' '.join(s for s in text.split() if s not in STOPWORDS)# delete stopwords from text  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(text_prepare(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'basically family little boy jake thinks zombie closet parents fighting time movie slower soap opera suddenly jake decides become rambo kill zombie ok first going make film must decide thriller drama drama movie watchable parents divorcing arguing like real life jake closet totally ruins film expected see boogeyman similar movie instead watched drama meaningless thriller spots 3 10 well playing parents descent dialogs shots jake ignore'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y=movie_reviews['sentiment']\n",
    "Y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "#print(X_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "words_counts = defaultdict(int)\n",
    "for text in X_train:\n",
    "    for word in text.split():\n",
    "        words_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:DICT_SIZE]\n",
    "WORDS_TO_INDEX = {p[0]:i for i,p in enumerate(most_common_words[:DICT_SIZE])}\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in text.split():\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] += 1\n",
    "    return result_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse\n",
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "\n",
    "\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (40000, 5000)\n",
      "X_test shape  (10000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 109 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mybag [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(token_pattern='(\\S+)', max_df= 0.9, min_df= 5, ngram_range=(1,2))\n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)   \n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    return X_train,X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf,X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dark1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8753"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_mybag,Y_train)\n",
    "\n",
    "Y_pred_mybag=model.predict(X_test_mybag)\n",
    "\n",
    "model.score(X_test_mybag,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9074"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tfidf,Y_train)\n",
    "\n",
    "Y_pred_tfidf=model.predict(X_test_tfidf)\n",
    "\n",
    "model.score(X_test_tfidf,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4426,  535],\n",
       "       [ 391, 4648]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred_tfidf)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4317,  644],\n",
       "       [ 603, 4436]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred_mybag)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8636"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(600)\n",
    "model.fit(X_train_mybag,Y_train)\n",
    "Y_pred_rf=model.predict(X_test_mybag)\n",
    "\n",
    "model.score(X_test_mybag,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8818"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(600)\n",
    "model.fit(X_train_tfidf,Y_train)\n",
    "Y_pred_tfidf=model.predict(X_test_tfidf)\n",
    "\n",
    "model.score(X_test_tfidf,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4363,  598],\n",
       "       [ 584, 4455]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred_tfidf)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4317,  644],\n",
       "       [ 603, 4436]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred_mybag)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latter day fulci schlocker totally abysmal concoction dealing incurable gambler brett halsey decides bluebeard style pay ever rising debts seducing ugliest bitches ever lay eyes happen wealthy widows fulci penned script also contrives incorporate blackly comedic elements result unfunny business involving corpse stay put opera singer victim stop singing etc mention doppelganger theme straight student prague although case two personas communicate via pre recorded radio messages end say surprised film shows sign sophistication mario bava hatchet honeymoon 1970 resembles several ways content merely pile disgustingly gory none convincing effects dismembered limbs squashed melting faces alas fulci become completely associated'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_counts)\n",
    "X_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_tk = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tk = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latter day fulci schlocker totally abysmal concoction dealing incurable gambler brett halsey decides bluebeard style pay ever rising debts seducing ugliest bitches ever lay eyes happen wealthy widows fulci penned script also contrives incorporate blackly comedic elements result unfunny business involving corpse stay put opera singer victim stop singing etc mention doppelganger theme straight student prague although case two personas communicate via pre recorded radio messages end say surprised film shows sign sophistication mario bava hatchet honeymoon 1970 resembles several ways content merely pile disgustingly gory none convincing effects dismembered limbs squashed melting faces alas fulci become completely associated\n"
     ]
    }
   ],
   "source": [
    "print(X_train[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jane austen would definitely approve one gwyneth paltrow awesome job capturing attitude emma funny without excessively silly yet elegant puts convincing british accent british maybe best judge fooled also excellent sliding doors sometimes forget american also brilliant jeremy northam sophie thompson phyllida law emma thompson sister mother bates women nearly steal show ms law even lines highly recommended\n"
     ]
    }
   ],
   "source": [
    "print(X_test[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[984,\n",
       " 3562,\n",
       " 8,\n",
       " 287,\n",
       " 3,\n",
       " 1017,\n",
       " 182,\n",
       " 4431,\n",
       " 1910,\n",
       " 3227,\n",
       " 60,\n",
       " 100,\n",
       " 535,\n",
       " 137,\n",
       " 4305,\n",
       " 1263,\n",
       " 946,\n",
       " 563,\n",
       " 1061,\n",
       " 563,\n",
       " 159,\n",
       " 38,\n",
       " 1626,\n",
       " 4084,\n",
       " 18,\n",
       " 202,\n",
       " 3333,\n",
       " 416,\n",
       " 706,\n",
       " 177,\n",
       " 18,\n",
       " 389,\n",
       " 3395,\n",
       " 4463,\n",
       " 961,\n",
       " 3227,\n",
       " 4463,\n",
       " 599,\n",
       " 268,\n",
       " 3970,\n",
       " 254,\n",
       " 660,\n",
       " 1938,\n",
       " 40,\n",
       " 1482,\n",
       " 961,\n",
       " 7,\n",
       " 293,\n",
       " 412,\n",
       " 995]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tk[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[984,\n",
       " 3562,\n",
       " 8,\n",
       " 287,\n",
       " 3,\n",
       " 1017,\n",
       " 182,\n",
       " 4431,\n",
       " 1910,\n",
       " 3227,\n",
       " 60,\n",
       " 100,\n",
       " 535,\n",
       " 137,\n",
       " 4305,\n",
       " 1263,\n",
       " 946,\n",
       " 563,\n",
       " 1061,\n",
       " 563,\n",
       " 159,\n",
       " 38,\n",
       " 1626,\n",
       " 4084,\n",
       " 18,\n",
       " 202,\n",
       " 3333,\n",
       " 416,\n",
       " 706,\n",
       " 177,\n",
       " 18,\n",
       " 389,\n",
       " 3395,\n",
       " 4463,\n",
       " 961,\n",
       " 3227,\n",
       " 4463,\n",
       " 599,\n",
       " 268,\n",
       " 3970,\n",
       " 254,\n",
       " 660,\n",
       " 1938,\n",
       " 40,\n",
       " 1482,\n",
       " 961,\n",
       " 7,\n",
       " 293,\n",
       " 412,\n",
       " 995]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tk[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=70\n",
    "X_train_tk = pad_sequences(X_train_tk, padding='post', maxlen=maxlen)\n",
    "X_test_tk = pad_sequences(X_test_tk, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1436,  143, 2531,  331, 4314, 1766,  944,  278,  850,   42, 4126,\n",
       "         42, 4053,  388,  439, 2743, 2531,  118,   18, 1560,  658,  806,\n",
       "       1873,  830, 1051, 3267,  637,  157, 1209, 1638, 1219,  413, 1021,\n",
       "        378,  618,  627,  583, 1211,  146,  294,   31, 2561, 1643, 3736,\n",
       "       1314, 3430,   45,   48,  632,    2,  160, 1874, 3546, 2546, 3655,\n",
       "        316,  635, 1319, 1457, 2445, 1937,  493,  946,  169, 1309, 2896,\n",
       "       2531,  315,  223, 3347])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tk[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(words_counts)\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 70, 100)           9269600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7001      \n",
      "=================================================================\n",
      "Total params: 9,276,601\n",
      "Trainable params: 7,001\n",
      "Non-trainable params: 9,269,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 1s 42us/step - loss: 0.5592 - acc: 0.7095 - val_loss: 0.4980 - val_acc: 0.7646\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 1s 37us/step - loss: 0.4599 - acc: 0.7861 - val_loss: 0.4855 - val_acc: 0.7700\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 1s 40us/step - loss: 0.4310 - acc: 0.8037 - val_loss: 0.4888 - val_acc: 0.7706\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 1s 42us/step - loss: 0.4132 - acc: 0.8142 - val_loss: 0.4932 - val_acc: 0.7679\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 1s 43us/step - loss: 0.4021 - acc: 0.8177 - val_loss: 0.4973 - val_acc: 0.7653\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 1s 39us/step - loss: 0.3946 - acc: 0.8230 - val_loss: 0.5355 - val_acc: 0.7546\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tk, Y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_tk, Y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation,Conv1D,MaxPooling2D,Flatten\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 18s 556us/step - loss: 0.4678 - acc: 0.7748 - val_loss: 0.4036 - val_acc: 0.8183\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 19s 581us/step - loss: 0.3468 - acc: 0.8504 - val_loss: 0.3543 - val_acc: 0.8441\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 19s 598us/step - loss: 0.2981 - acc: 0.8754 - val_loss: 0.3476 - val_acc: 0.8472\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 19s 598us/step - loss: 0.2506 - acc: 0.9035 - val_loss: 0.3406 - val_acc: 0.8511\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 20s 617us/step - loss: 0.2120 - acc: 0.9247 - val_loss: 0.3458 - val_acc: 0.8501\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 19s 597us/step - loss: 0.1763 - acc: 0.9431 - val_loss: 0.3470 - val_acc: 0.8510\n",
      "10000/10000 [==============================] - 2s 211us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tk, Y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test_tk, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.34299210641384126\n",
      "Test Accuracy: 0.8517000079154968\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from keras.layers import Embedding,LSTM\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 70, 100)           9269600   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,386,977\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 9,269,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 70s 2ms/step - loss: 0.5046 - acc: 0.7547 - val_loss: 0.4121 - val_acc: 0.8165\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 69s 2ms/step - loss: 0.3999 - acc: 0.8184 - val_loss: 0.3797 - val_acc: 0.8404\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 70s 2ms/step - loss: 0.3627 - acc: 0.8392 - val_loss: 0.3567 - val_acc: 0.8428\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 71s 2ms/step - loss: 0.3314 - acc: 0.8566 - val_loss: 0.3288 - val_acc: 0.8583\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 75s 2ms/step - loss: 0.3088 - acc: 0.8685 - val_loss: 0.3234 - val_acc: 0.8619\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 72s 2ms/step - loss: 0.2936 - acc: 0.8747 - val_loss: 0.3291 - val_acc: 0.8614\n",
      "10000/10000 [==============================] - 11s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tk, Y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test_tk, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.3290853905677795\n",
      "Test Accuracy: 0.8532999753952026\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
